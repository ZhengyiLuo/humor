{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import glob\n",
    "import os\n",
    "import sys\n",
    "import pdb\n",
    "import os.path as osp\n",
    "sys.path.append(os.getcwd())\n",
    "import importlib, time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from humor.utils.config import TestConfig\n",
    "from humor.utils.logging import Logger, class_name_to_file_name, mkdir, cp_files\n",
    "from humor.utils.torch import get_device, save_state, load_state\n",
    "from humor.utils.stats import StatTracker\n",
    "from humor.utils.transforms import rotation_matrix_to_angle_axis\n",
    "from humor.body_model.utils import SMPL_JOINTS\n",
    "from humor.datasets.amass_utils import NUM_KEYPT_VERTS, CONTACT_INDS\n",
    "from humor.losses.humor_loss import CONTACT_THRESH\n",
    "\n",
    "NUM_WORKERS = 0\n",
    "\n",
    "def parse_args(argv):\n",
    "    # create config and parse args\n",
    "    config = TestConfig(argv)\n",
    "    known_args, unknown_args = config.parse()\n",
    "    print('Unrecognized args: ' + str(unknown_args))\n",
    "    return known_args\n",
    "\n",
    "def test(args_obj, config_file):\n",
    "\n",
    "    # set up output\n",
    "    args = args_obj.base\n",
    "    mkdir(args.out)\n",
    "\n",
    "    # create logging system\n",
    "    test_log_path = os.path.join(args.out, 'test.log')\n",
    "    Logger.init(test_log_path)\n",
    "\n",
    "    # save arguments used\n",
    "    Logger.log('Base args: ' + str(args))\n",
    "    Logger.log('Model args: ' + str(args_obj.model))\n",
    "    Logger.log('Dataset args: ' + str(args_obj.dataset))\n",
    "    Logger.log('Loss args: ' + str(args_obj.loss))\n",
    "\n",
    "    # save training script/model/dataset/config used\n",
    "    test_scripts_path = os.path.join(args.out, 'test_scripts')\n",
    "    mkdir(test_scripts_path)\n",
    "    pkg_root = os.path.join(cur_file_path, '..')\n",
    "    dataset_file = class_name_to_file_name(args.dataset)\n",
    "    dataset_file_path = os.path.join(pkg_root, 'datasets/' + dataset_file + '.py')\n",
    "    model_file = class_name_to_file_name(args.model)\n",
    "    loss_file = class_name_to_file_name(args.loss)\n",
    "    model_file_path = os.path.join(pkg_root, 'models/' + model_file + '.py')\n",
    "    train_file_path = os.path.join(pkg_root, 'test/test_humor.py')\n",
    "    cp_files(test_scripts_path, [train_file_path, model_file_path, dataset_file_path, config_file])\n",
    "\n",
    "    # load model class and instantiate\n",
    "    model_class = importlib.import_module('models.' + model_file)\n",
    "    Model = getattr(model_class, args.model)\n",
    "    model = Model(**args_obj.model_dict,\n",
    "                    model_smpl_batch_size=args.batch_size) # assumes model is HumorModel\n",
    "\n",
    "    # load loss class and instantiate\n",
    "    loss_class = importlib.import_module('losses.' + loss_file)\n",
    "    Loss = getattr(loss_class, args.loss)\n",
    "    loss_func = Loss(**args_obj.loss_dict,\n",
    "                      smpl_batch_size=args.batch_size*args_obj.dataset.sample_num_frames) # assumes loss is HumorLoss\n",
    "\n",
    "    device = get_device(args.gpu)\n",
    "    model.to(device)\n",
    "    loss_func.to(device)\n",
    "\n",
    "    print(model)\n",
    "\n",
    "    # count params\n",
    "    model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "    Logger.log('Num model params: ' + str(params))\n",
    "\n",
    "    # freeze params in loss\n",
    "    for param in loss_func.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # load in pretrained weights if given\n",
    "    if args.ckpt is not None:\n",
    "        start_epoch, min_val_loss, min_train_loss = load_state(args.ckpt, model, optimizer=None, map_location=device, ignore_keys=model.ignore_keys)\n",
    "        Logger.log('Successfully loaded saved weights...')\n",
    "        Logger.log('Saved checkpoint is from epoch idx %d with min val loss %.6f...' % (start_epoch, min_val_loss))\n",
    "    else:\n",
    "        Logger.log('ERROR: No weight specified to load!!')\n",
    "        # return\n",
    "\n",
    "    # load dataset class and instantiate training and validation set\n",
    "    if args.test_on_train:\n",
    "        Logger.log('WARNING: running evaluation on TRAINING data as requested...should only be used for debugging!')\n",
    "    elif args.test_on_val:\n",
    "        Logger.log('WARNING: running evaluation on VALIDATION data as requested...should only be used for debugging!')\n",
    "    Dataset = getattr(importlib.import_module('datasets.' + dataset_file), args.dataset)\n",
    "    split = 'test'\n",
    "    if args.test_on_train:\n",
    "        split = 'train'\n",
    "    elif args.test_on_val:\n",
    "        split = 'val'\n",
    "    test_dataset = Dataset(split=split, **args_obj.dataset_dict)\n",
    "    # create loaders\n",
    "    test_loader = DataLoader(test_dataset, \n",
    "                            batch_size=args.batch_size,\n",
    "                            shuffle=args.shuffle_test, \n",
    "                            num_workers=NUM_WORKERS,\n",
    "                            pin_memory=True,\n",
    "                            drop_last=False,\n",
    "                            worker_init_fn=lambda _: np.random.seed())\n",
    "\n",
    "    test_dataset.return_global = True\n",
    "    model.dataset = test_dataset\n",
    "\n",
    "    if args.eval_full_test:\n",
    "        Logger.log('Running full test set evaluation...')\n",
    "        # stats tracker\n",
    "        tensorboard_path = os.path.join(args.out, 'test_tensorboard')\n",
    "        mkdir(tensorboard_path)\n",
    "        stat_tracker = StatTracker(tensorboard_path)\n",
    "\n",
    "        # testing with same stats as training\n",
    "        test_start_t = time.time()\n",
    "        test_dataset.pre_batch()\n",
    "        model.eval()\n",
    "        for i, data in enumerate(test_loader):\n",
    "            batch_start_t = time.time()\n",
    "            # run model\n",
    "            #   note we're always using ground truth input so this is only measuring single-step error, just like in training\n",
    "            loss, stats_dict = model_class.step(model, loss_func, data, test_dataset, device, 0, mode='test', use_gt_p=1.0)\n",
    "\n",
    "            # collect stats\n",
    "            batch_elapsed_t = time.time() - batch_start_t\n",
    "            total_elapsed_t = time.time() - test_start_t\n",
    "            stats_dict['loss'] = loss\n",
    "            stats_dict['time_per_batch'] = torch.Tensor([batch_elapsed_t])[0]\n",
    "\n",
    "            stat_tracker.update(stats_dict, tag='test')\n",
    "\n",
    "            if i % args.print_every == 0:\n",
    "                stat_tracker.print(i, len(test_loader),\n",
    "                                0, 1,\n",
    "                                total_elapsed_time=total_elapsed_t,\n",
    "                                tag='test')\n",
    "\n",
    "            test_dataset.pre_batch()\n",
    "\n",
    "    if args.eval_sampling or args.eval_sampling_debug:\n",
    "        eval_sampling(model, test_dataset, test_loader, device,\n",
    "                            out_dir=args.out if args.eval_sampling else None,\n",
    "                            num_samples=args.eval_num_samples,\n",
    "                            samp_len=args.eval_sampling_len,\n",
    "                            viz_contacts=args.viz_contacts,\n",
    "                            viz_pred_joints=args.viz_pred_joints,\n",
    "                            viz_smpl_joints=args.viz_smpl_joints)\n",
    "\n",
    "    Logger.log('Finished!')\n",
    "\n",
    "def eval_sampling(model, test_dataset, test_loader, device, \n",
    "                  out_dir=None,\n",
    "                  num_samples=1,\n",
    "                  samp_len=10.0,\n",
    "                  viz_contacts=False,\n",
    "                  viz_pred_joints=False,\n",
    "                  viz_smpl_joints=False):\n",
    "    Logger.log('Evaluating sampling qualitatively...')\n",
    "    from body_model.body_model import BodyModel\n",
    "    from body_model.utils import SMPLH_PATH\n",
    "\n",
    "    eval_qual_samp_len = int(samp_len * 30.0) # at 30 Hz\n",
    "\n",
    "    res_out_dir = None\n",
    "    if out_dir is not None:\n",
    "        res_out_dir = os.path.join(out_dir, 'eval_sampling')\n",
    "        if not os.path.exists(res_out_dir):\n",
    "            os.mkdir(res_out_dir)\n",
    "\n",
    "    J = len(SMPL_JOINTS)\n",
    "    V = NUM_KEYPT_VERTS\n",
    "    male_bm_path = os.path.join(SMPLH_PATH, 'male/model.npz')\n",
    "    female_bm_path = os.path.join(SMPLH_PATH, 'female/model.npz')\n",
    "    male_bm = BodyModel(bm_path=male_bm_path, num_betas=16, batch_size=eval_qual_samp_len).to(device)\n",
    "    female_bm = BodyModel(bm_path=female_bm_path, num_betas=16, batch_size=eval_qual_samp_len).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        test_dataset.pre_batch()\n",
    "        model.eval()\n",
    "        for i, data in enumerate(test_loader):\n",
    "            # get inputs\n",
    "            batch_in, batch_out, meta = data\n",
    "            print(meta['path'])\n",
    "            seq_name_list = [spath[:-4] for spath in meta['path']]\n",
    "            if res_out_dir is None:\n",
    "                batch_res_out_list = [None]*len(seq_name_list)\n",
    "            else:\n",
    "                batch_res_out_list = [os.path.join(res_out_dir, seq_name.replace('/', '_') + '_b' + str(i) + 'seq' + str(sidx)) for sidx, seq_name in enumerate(seq_name_list)]\n",
    "                print(batch_res_out_list)\n",
    "            # continue\n",
    "            x_past, _, gt_dict, input_dict, global_gt_dict = model.prepare_input(batch_in, device, \n",
    "                                                                                data_out=batch_out,\n",
    "                                                                                return_input_dict=True,\n",
    "                                                                                return_global_dict=True)\n",
    "\n",
    "            # roll out predicted motion\n",
    "            B, T, _, _ = x_past.size()\n",
    "            x_past = x_past[:,0,:,:] # only need input for first step\n",
    "            rollout_input_dict = dict()\n",
    "            for k in input_dict.keys():\n",
    "                rollout_input_dict[k] = input_dict[k][:,0,:,:] # only need first step\n",
    "\n",
    "            # sample same trajectory multiple times and save the joints/contacts output\n",
    "            for samp_idx in range(num_samples):\n",
    "                x_pred_dict = model.roll_out(x_past, rollout_input_dict, eval_qual_samp_len, gender=meta['gender'], betas=meta['betas'].to(device))\n",
    "\n",
    "                # visualize and save\n",
    "                print('Visualizing sample %d/%d!' % (samp_idx+1, num_samples))\n",
    "                imsize = (1080, 1080)\n",
    "                cur_res_out_list = batch_res_out_list\n",
    "                if res_out_dir is not None:\n",
    "                    cur_res_out_list = [out_path + '_samp%d' % (samp_idx) for out_path in batch_res_out_list]\n",
    "                    imsize = (720, 720)\n",
    "                viz_eval_samp(global_gt_dict, x_pred_dict, meta, male_bm, female_bm, cur_res_out_list,\n",
    "                                imw=imsize[0],\n",
    "                                imh=imsize[1],\n",
    "                                show_smpl_joints=viz_smpl_joints,\n",
    "                                show_pred_joints=viz_pred_joints,\n",
    "                                show_contacts=viz_contacts\n",
    "                              )\n",
    "\n",
    "def viz_eval_samp(global_gt_dict, x_pred_dict, meta, male_bm, female_bm, out_path_list,\n",
    "                    imw=720,\n",
    "                    imh=720,\n",
    "                    show_pred_joints=False,\n",
    "                    show_smpl_joints=False,\n",
    "                    show_contacts=False):\n",
    "    '''\n",
    "    Given x_pred_dict from the model rollout and the ground truth dict, runs through SMPL model to visualize\n",
    "    '''\n",
    "    J = len(SMPL_JOINTS)\n",
    "    V = NUM_KEYPT_VERTS\n",
    "\n",
    "    pred_world_root_orient = x_pred_dict['root_orient']\n",
    "    B, T, _ = pred_world_root_orient.size()\n",
    "    pred_world_root_orient = rotation_matrix_to_angle_axis(pred_world_root_orient.reshape((B*T, 3, 3))).reshape((B, T, 3))\n",
    "    pred_world_pose_body = x_pred_dict['pose_body']\n",
    "    pred_world_pose_body = rotation_matrix_to_angle_axis(pred_world_pose_body.reshape((B*T*(J-1), 3, 3))).reshape((B, T, (J-1)*3))\n",
    "    pred_world_trans = x_pred_dict['trans']\n",
    "    pred_world_joints = x_pred_dict['joints'].reshape((B, T, J, 3))\n",
    "\n",
    "    viz_contacts = [None]*B\n",
    "    if show_contacts and 'contacts' in x_pred_dict.keys():\n",
    "        pred_contacts = torch.sigmoid(x_pred_dict['contacts'])\n",
    "        pred_contacts = (pred_contacts > CONTACT_THRESH).to(torch.float)\n",
    "        viz_contacts = torch.zeros((B, T, len(SMPL_JOINTS))).to(pred_contacts)\n",
    "        viz_contacts[:,:,CONTACT_INDS] = pred_contacts\n",
    "        pred_contacts = viz_contacts\n",
    "\n",
    "    betas = meta['betas'].to(global_gt_dict[list(global_gt_dict.keys())[0]].device)\n",
    "    for b in range(B):\n",
    "        bm_world = male_bm if meta['gender'][b] == 'male' else female_bm\n",
    "        # pred\n",
    "        body_pred = bm_world(pose_body=pred_world_pose_body[b], \n",
    "                        pose_hand=None,\n",
    "                        betas=betas[b,0].reshape((1, -1)).expand((T, 16)),\n",
    "                        root_orient=pred_world_root_orient[b],\n",
    "                        trans=pred_world_trans[b])\n",
    "\n",
    "        pred_smpl_joints = body_pred.Jtr[:, :J]\n",
    "        viz_joints = None\n",
    "        if show_smpl_joints:\n",
    "            viz_joints = pred_smpl_joints\n",
    "        elif show_pred_joints:\n",
    "            viz_joints = pred_world_joints[b]\n",
    "\n",
    "        cur_offscreen = out_path_list[b] is not None\n",
    "        from viz.utils import viz_smpl_seq, create_video\n",
    "        body_alpha = 0.5 if viz_joints is not None and cur_offscreen else 1.0\n",
    "        viz_smpl_seq(body_pred,\n",
    "                        imw=imw, imh=imh, fps=30,\n",
    "                        render_body=True,\n",
    "                        render_joints=viz_joints is not None,\n",
    "                        render_skeleton=viz_joints is not None and cur_offscreen,\n",
    "                        render_ground=True,\n",
    "                        contacts=viz_contacts[b],\n",
    "                        joints_seq=viz_joints,\n",
    "                        body_alpha=body_alpha,\n",
    "                        use_offscreen=cur_offscreen,\n",
    "                        out_path=out_path_list[b],\n",
    "                        wireframe=False,\n",
    "                        RGBA=False,\n",
    "                        follow_camera=True,\n",
    "                        cam_offset=[0.0, 2.2, 0.9],\n",
    "                        joint_color=[ 0.0, 1.0, 0.0 ],\n",
    "                        point_color=[0.0, 0.0, 1.0],\n",
    "                        skel_color=[0.5, 0.5, 0.5],\n",
    "                        joint_rad=0.015,\n",
    "                        point_rad=0.015\n",
    "                )\n",
    "\n",
    "        if cur_offscreen:\n",
    "            create_video(out_path_list[b] + '/frame_%08d.' + '%s' % ('png'), out_path_list[b] + '.mp4', 30)\n",
    "\n",
    "\n",
    "def main(args, config_file):\n",
    "    test(args, config_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unrecognized args: []\n"
     ]
    }
   ],
   "source": [
    "args_obj = parse_args(['@./configs/test_humor_sampling.cfg'])\n",
    "config_file = './configs/test_humor_sampling.cfg'\n",
    "args = args_obj.base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using posterior architecture: mlp\n",
      "Using decoder architecture: mlp\n",
      "Using prior architecture: mlp\n",
      "Using detected GPU...\n",
      "HumorModel(\n",
      "  (encoder): MLP(\n",
      "    (net): ModuleList(\n",
      "      (0): Linear(in_features=678, out_features=1024, bias=True)\n",
      "      (1): GroupNorm(16, 1024, eps=1e-05, affine=True)\n",
      "      (2): ReLU()\n",
      "      (3): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (4): GroupNorm(16, 1024, eps=1e-05, affine=True)\n",
      "      (5): ReLU()\n",
      "      (6): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (7): GroupNorm(16, 1024, eps=1e-05, affine=True)\n",
      "      (8): ReLU()\n",
      "      (9): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (10): GroupNorm(16, 1024, eps=1e-05, affine=True)\n",
      "      (11): ReLU()\n",
      "      (12): Linear(in_features=1024, out_features=96, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (decoder): MLP(\n",
      "    (net): ModuleList(\n",
      "      (0): Linear(in_features=387, out_features=1024, bias=True)\n",
      "      (1): GroupNorm(16, 1024, eps=1e-05, affine=True)\n",
      "      (2): ReLU()\n",
      "      (3): Linear(in_features=1072, out_features=1024, bias=True)\n",
      "      (4): GroupNorm(16, 1024, eps=1e-05, affine=True)\n",
      "      (5): ReLU()\n",
      "      (6): Linear(in_features=1072, out_features=512, bias=True)\n",
      "      (7): GroupNorm(16, 512, eps=1e-05, affine=True)\n",
      "      (8): ReLU()\n",
      "      (9): Linear(in_features=560, out_features=216, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (prior_net): MLP(\n",
      "    (net): ModuleList(\n",
      "      (0): Linear(in_features=339, out_features=1024, bias=True)\n",
      "      (1): GroupNorm(16, 1024, eps=1e-05, affine=True)\n",
      "      (2): ReLU()\n",
      "      (3): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (4): GroupNorm(16, 1024, eps=1e-05, affine=True)\n",
      "      (5): ReLU()\n",
      "      (6): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (7): GroupNorm(16, 1024, eps=1e-05, affine=True)\n",
      "      (8): ReLU()\n",
      "      (9): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (10): GroupNorm(16, 1024, eps=1e-05, affine=True)\n",
      "      (11): ReLU()\n",
      "      (12): Linear(in_features=1024, out_features=96, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Num model params: 9725976\n",
      "Successfully loaded saved weights...\n",
      "Saved checkpoint is from epoch idx 196 with min val loss 0.017165...\n",
      "Loading data from['./data/amass_processed']\n",
      "Logger must be initialized before logging!\n",
      "Found the following datasets for this split:\n",
      "['./data/amass_processed/MPI_HDM05', './data/amass_processed/SFU', './data/amass_processed/MPI_mosh']\n",
      "This split contains 324 sequences (that meet the duration criteria).\n",
      "Logger must be initialized before logging!\n",
      "The dataset contains 22554 sub-sequences in total.\n",
      "Logger must be initialized before logging!\n"
     ]
    }
   ],
   "source": [
    "import humor.models.humor_model as HumorModel\n",
    "from humor.losses.humor_loss import HumorLoss\n",
    "from humor.datasets.amass_discrete_dataset import AmassDiscreteDataset\n",
    "\n",
    "# pkg_root = os.path.join('..')\n",
    "# model_file_path = os.path.join(pkg_root, 'models/' + \"humor_model\" + '.py')\n",
    "# train_file_path = os.path.join(pkg_root, 'test/test_humor.py')\n",
    "# cp_files(test_scripts_path, [train_file_path, model_file_path, dataset_file_path, config_file])\n",
    "\n",
    "# load model class and instantiate\n",
    "model_class = HumorModel\n",
    "model = HumorModel.HumorModel(**args_obj.model_dict,\n",
    "                model_smpl_batch_size=args.batch_size) # assumes model is HumorModel\n",
    "\n",
    "# load loss class and instantiate\n",
    "loss_func = HumorLoss(**args_obj.loss_dict,\n",
    "                  smpl_batch_size=args.batch_size*args_obj.dataset.sample_num_frames) # assumes loss is HumorLoss\n",
    "\n",
    "device = get_device(args.gpu)\n",
    "model.to(device)\n",
    "loss_func.to(device)\n",
    "\n",
    "print(model)\n",
    "\n",
    "# count params\n",
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "print('Num model params: ' + str(params))\n",
    "\n",
    "# freeze params in loss\n",
    "for param in loss_func.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# load in pretrained weights if given\n",
    "if args.ckpt is not None:\n",
    "    start_epoch, min_val_loss, min_train_loss = load_state(args.ckpt, model, optimizer=None, map_location=device, ignore_keys=model.ignore_keys)\n",
    "    print('Successfully loaded saved weights...')\n",
    "    print('Saved checkpoint is from epoch idx %d with min val loss %.6f...' % (start_epoch, min_val_loss))\n",
    "else:\n",
    "    print('ERROR: No weight specified to load!!')\n",
    "    # return\n",
    "\n",
    "# load dataset class and instantiate training and validation set\n",
    "if args.test_on_train:\n",
    "    print('WARNING: running evaluation on TRAINING data as requested...should only be used for debugging!')\n",
    "elif args.test_on_val:\n",
    "    print('WARNING: running evaluation on VALIDATION data as requested...should only be used for debugging!')\n",
    "split = 'val'\n",
    "# if args.test_on_train:\n",
    "#     split = 'train'\n",
    "# elif args.test_on_val:\n",
    "#     split = 'val'\n",
    "test_dataset = AmassDiscreteDataset(split=split, **args_obj.dataset_dict)\n",
    "# create loaders\n",
    "test_loader = DataLoader(test_dataset, \n",
    "                        batch_size=args.batch_size,\n",
    "                        shuffle=args.shuffle_test, \n",
    "                        num_workers=NUM_WORKERS,\n",
    "                        pin_memory=True,\n",
    "                        drop_last=False,\n",
    "                        worker_init_fn=lambda _: np.random.seed())\n",
    "\n",
    "test_dataset.return_global = True\n",
    "model.dataset = test_dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Step Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0024, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "test_dataset.pre_batch()\n",
    "model.eval()\n",
    "for i, data in enumerate(test_loader):\n",
    "    batch_start_t = time.time()\n",
    "    # run model\n",
    "    #   note we're always using ground truth input so this is only measuring single-step error, just like in training\n",
    "    loss, stats_dict = model_class.step(model, loss_func, data, test_dataset, device, 0, mode='test', use_gt_p=1.0)\n",
    "    print(loss)\n",
    "    \n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['pose_body', 'root_orient', 'root_orient_vel', 'trans', 'trans_vel', 'joints', 'joints_vel', 'contacts'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 1, 189])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]['pose_body'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling for sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, data in enumerate(test_loader):\n",
    "    batch_in, batch_out, meta = data\n",
    "    break\n",
    "data_names = ['trans', 'trans_vel', 'root_orient', 'root_orient_vel', 'pose_body', 'joints', 'joints_vel', 'contacts']\n",
    "x_past, _, gt_dict, input_dict, global_gt_dict = model.prepare_input(\n",
    "                batch_in,\n",
    "                device,\n",
    "                data_out=batch_out,\n",
    "                return_input_dict=True,\n",
    "                return_global_dict=True,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rollout_input_dict = dict()\n",
    "# for k in input_dict.keys():\n",
    "#     rollout_input_dict[k] = input_dict[k][\n",
    "#         :, 0, :, :\n",
    "#     ]  # only need first step\n",
    "\n",
    "# eval_qual_samp_len = 1\n",
    "# x_pred_dict = model.roll_out(\n",
    "#                     x_past[0, 0:1],\n",
    "#                     rollout_input_dict,\n",
    "#                     eval_qual_samp_len,\n",
    "#                     gender=meta[\"gender\"],\n",
    "#                     betas=meta[\"betas\"].to(device),\n",
    "#                 )\n",
    "\n",
    "sample_out = model.sample_step(x_past[0, 0])\n",
    "decoder_out = sample_out['decoder_out']\n",
    "x_pred_dict = model.split_output(decoder_out, convert_rots=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: You are using a SMPL+H model, with only 10 shape coefficients.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SMPLH_Parser(\n",
       "  Gender: MALE\n",
       "  Number of joints: 52\n",
       "  Betas: 16\n",
       "  Flat hand mean: False\n",
       "  (vertex_joint_selector): VertexJointSelector()\n",
       ")"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from copycat.utils.transform_utils import (\n",
    "    convert_aa_to_orth6d, convert_orth_6d_to_aa, vertizalize_smpl_root,\n",
    "    rotation_matrix_to_angle_axis, rot6d_to_rotmat, convert_orth_6d_to_mat, angle_axis_to_rotation_matrix,\n",
    "    angle_axis_to_quaternion\n",
    ")\n",
    "from copycat.smpllib.smpl_parser import SMPL_Parser, SMPL_BONE_ORDER_NAMES, SMPLH_Parser\n",
    "device_cpu = torch.device(\"cpu\")\n",
    "# smpl_p = SMPL_Parser(\"/hdd/zen/dev/copycat/Copycat/data/smpl\", gender = \"male\")\n",
    "# smpl_p.to(device_cpu)\n",
    "\n",
    "smplh_p = SMPLH_Parser(\"/hdd/zen/dev/copycat/Copycat/data/smpl\", gender = \"male\", use_pca = False, create_transl = False)\n",
    "smplh_p.to(device_cpu)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyvista as pv\n",
    "pose_aa_body = rotation_matrix_to_angle_axis(x_pred_dict['pose_body'].reshape(21, 3, 3))\n",
    "pose_aa = torch.cat([rotation_matrix_to_angle_axis(x_pred_dict['root_orient'].reshape(1, 3, 3)), pose_aa_body, torch.zeros((30, 3)).to(device)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spose_aa_body = rotation_matrix_to_angle_axis(input_dict['pose_body'][0,0].reshape(21, 3, 3))\n",
    "# pose_aa_prev = torch.cat([rotation_matrix_to_angle_axis(input_dict['root_orient'][0,0].reshape(1, 3, 3)), pose_aa_body, torch.zeros((30, 3)).to(device)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50fcceb196ff4280a177734d5af83313",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Renderer(camera=PerspectiveCamera(aspect=1.3333333333333333, children=(DirectionalLight(color='#fefefe', inten…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    pose = pose_aa\n",
    "#     pose[:, :3] = 0\n",
    "    betas = torch.zeros((1, 16))\n",
    "    verts, jts = smplh_p.get_joints_verts(pose.cpu(), betas.cpu())\n",
    "    vertices = verts[0].numpy()\n",
    "    \n",
    "    verts_prev, jts = smplh_p.get_joints_verts(pose_aa_prev.cpu(), betas.cpu())\n",
    "    vertices_prev = verts_prev[0].numpy()\n",
    "    # mesh faces\n",
    "    faces = smplh_p.faces\n",
    "    faces =  np.hstack([np.concatenate([[3], f]) for f in faces])\n",
    "    mesh = pv.PolyData(vertices, faces = faces)\n",
    "    mesh_prev = pv.PolyData(vertices_prev, faces = faces)\n",
    "    \n",
    "#     mesh.plot( jupyter_backend='pythreejs')\n",
    "#     pv.plot([mesh, mesh], jupyter_backend='pythreejs')\n",
    "pl = pv.Plotter()\n",
    "plane = pv.Plane( i_size=5, j_size=5, i_resolution=10, j_resolution=10)\n",
    "pl.add_mesh(mesh, show_edges=True, color='yellow')\n",
    "pl.add_mesh(mesh_prev, show_edges=True, color='red')\n",
    "pl.add_mesh(plane, show_edges=True, color='white')\n",
    "pl.show(jupyter_backend='pythreejs', cpos=[-1, 1, 0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 50, 9])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_pred_dict['contacts'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "test_data = joblib.load('test.pkl')\n",
    "\n",
    "acc_data, x_pred_dict = test_data[0], test_data[1]\n",
    "i = 0\n",
    "def dict_to_data(x_pred_dict):\n",
    "    B = x_pred_dict['pose_body'].squeeze().shape[0]\n",
    "    pose_aa_body = rotation_matrix_to_angle_axis(x_pred_dict['pose_body'].squeeze().reshape(B * 21, 3, 3)).reshape(B, 21, 3)\n",
    "    root_pose = rotation_matrix_to_angle_axis(x_pred_dict['root_orient'].squeeze().reshape(B, 3, 3)).reshape(B, 1,  3)\n",
    "    pose_aa = torch.cat([root_pose, pose_aa_body, torch.zeros((B, 30, 3)).to(device)], dim = 1)\n",
    "    trans = x_pred_dict['trans'].squeeze()\n",
    "    return pose_aa, trans\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49c750f488d542ae9330cc55fcba3e01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Renderer(camera=PerspectiveCamera(aspect=1.3333333333333333, children=(DirectionalLight(color='#fefefe', inten…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "faces = smplh_p.faces\n",
    "faces =  np.hstack([np.concatenate([[3], f]) for f in faces])\n",
    "\n",
    "pl = pv.Plotter()\n",
    "plane = pv.Plane( i_size=5, j_size=5, i_resolution=10, j_resolution=10)\n",
    "\n",
    "pose_aa, trans = dict_to_data(x_pred_dict)\n",
    "B = pose_aa.shape[0]\n",
    "with torch.no_grad():\n",
    "    pose = pose_aa\n",
    "    \n",
    "    betas = torch.zeros((B, 16))\n",
    "    verts, jts = smplh_p.get_joints_verts(pose.cpu(), betas.cpu(), trans.cpu())\n",
    "    for i in range(verts.shape[0]):\n",
    "        vertices = verts[i].numpy()\n",
    "        mesh = pv.PolyData(vertices, faces = faces)\n",
    "        pl.add_mesh(mesh, show_edges=True, color='yellow')\n",
    "\n",
    "\n",
    "x_pred_dict_acc = defaultdict(list)\n",
    "for data_entry in acc_data:\n",
    "    [x_pred_dict_acc[k].append(v.cpu().numpy()) for k, v in data_entry.items()]\n",
    "x_pred_dict_acc = {k: torch.from_numpy(np.array(v)).to(device) for k, v in x_pred_dict_acc.items()}\n",
    "pose_aa, trans = dict_to_data(x_pred_dict_acc)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    pose = pose_aa\n",
    "    betas = torch.zeros((B, 16))\n",
    "    verts, jts = smplh_p.get_joints_verts(pose.cpu(), betas.cpu(), trans.cpu())\n",
    "    for i in range(verts.shape[0]):\n",
    "        vertices = verts[i].numpy()\n",
    "        mesh = pv.PolyData(vertices, faces = faces)\n",
    "        pl.add_mesh(mesh, show_edges=True, color='red')\n",
    "\n",
    "\n",
    "pl.add_mesh(plane, show_edges=True, color='white')\n",
    "pl.show(jupyter_backend='pythreejs', cpos=[-1, 1, 0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: You are using a SMPL model, with only 10 shape coefficients.\n",
      "WARNING: You are using a SMPL+H model, with only 10 shape coefficients.\n"
     ]
    }
   ],
   "source": [
    "from copycat.smpllib.smpl_parser import SMPL_Parser, SMPLH_Parser\n",
    "smpl_p = SMPL_Parser(\"/hdd/zen/dev/copycat/Copycat/data/smpl\", gender=\"neutral\")\n",
    "smpl_hp = SMPLH_Parser(\"/hdd/zen/dev/copycat/Copycat/data/smpl\", gender=\"neutral\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "pose_aa_body = rotation_matrix_to_angle_axis(x_pred_dict['pose_body'].reshape(-1, 3, 3)).reshape(50, 21, 3)\n",
    "with torch.no_grad():\n",
    "    verts, jts = smplh_p.get_joints_verts(pose.cpu(), betas.cpu(), trans.cpu())\n",
    "    root_pose = rotation_matrix_to_angle_axis(x_pred_dict['root_orient'].squeeze().reshape(B, 3, 3)).reshape(B, 1,  3)\n",
    "    pose_aa = torch.cat([root_pose, pose_aa_body, torch.zeros((B, 2, 3)).to(device)], dim = 1)\n",
    "    verts_h, jts_h = smpl_p.get_joints_verts(pose_aa.cpu(), betas.cpu(), trans.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e9f91599224490c94952adfacd6eccc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Renderer(camera=PerspectiveCamera(aspect=1.3333333333333333, children=(DirectionalLight(color='#fefefe', inten…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pl = pv.Plotter()\n",
    "plane = pv.Plane( i_size=5, j_size=5, i_resolution=10, j_resolution=10)\n",
    "\n",
    "vertices = verts[0].numpy()\n",
    "mesh = pv.PolyData(vertices, faces = faces)\n",
    "pl.add_mesh(mesh, show_edges=True, color='red')\n",
    "\n",
    "\n",
    "vertices = verts_h[0].numpy()\n",
    "mesh = pv.PolyData(vertices, faces = faces)\n",
    "pl.add_mesh(mesh, show_edges=True, color='yellow')\n",
    "\n",
    "\n",
    "pl.add_mesh(plane, show_edges=True, color='white')\n",
    "pl.show(jupyter_backend='pythreejs', cpos=[-1, 1, 0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cv] *",
   "language": "python",
   "name": "conda-env-cv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
